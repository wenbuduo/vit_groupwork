# Vision Transformer (ViT) å¤ç°æŒ‡å—

## ğŸ“‹ ç›®å½•
1. [ç¯å¢ƒé…ç½®](#ç¯å¢ƒé…ç½®)
2. [å¿«é€Ÿå¼€å§‹](#å¿«é€Ÿå¼€å§‹)
3. [ä»é›¶å®ç°](#ä»é›¶å®ç°)
4. [è®­ç»ƒè‡ªå·±çš„æ¨¡å‹](#è®­ç»ƒè‡ªå·±çš„æ¨¡å‹)
5. [å¸¸è§é—®é¢˜](#å¸¸è§é—®é¢˜)

---

## ğŸ”§ ç¯å¢ƒé…ç½®

### æ–¹æ³•1: ä½¿ç”¨condaï¼ˆæ¨èï¼‰
```bash
# åˆ›å»ºè™šæ‹Ÿç¯å¢ƒ
conda create -n vit python=3.9
conda activate vit

# å®‰è£…PyTorchï¼ˆæ ¹æ®ä½ çš„CUDAç‰ˆæœ¬é€‰æ‹©ï¼‰
# CPUç‰ˆæœ¬
conda install pytorch torchvision torchaudio cpuonly -c pytorch

# GPUç‰ˆæœ¬ (CUDA 11.8)
conda install pytorch torchvision torchaudio pytorch-cuda=11.8 -c pytorch -c nvidia

# å®‰è£…å…¶ä»–ä¾èµ–
pip install transformers pillow requests tqdm matplotlib
```

### æ–¹æ³•2: ä½¿ç”¨pip
```bash
# åˆ›å»ºè™šæ‹Ÿç¯å¢ƒ
python -m venv vit_env
source vit_env/bin/activate  # Linux/Mac
# vit_env\Scripts\activate  # Windows

# å®‰è£…ä¾èµ–
pip install torch torchvision transformers pillow requests tqdm matplotlib
```

---

## ğŸš€ å¿«é€Ÿå¼€å§‹

### æ–¹æ¡ˆA: ä½¿ç”¨é¢„è®­ç»ƒæ¨¡å‹æ¨ç†

```bash
python vit_quickstart.py
```

**è¿™ä¸ªè„šæœ¬ä¼šï¼š**
- è‡ªåŠ¨ä¸‹è½½é¢„è®­ç»ƒçš„ViTæ¨¡å‹
- ä¸‹è½½æµ‹è¯•å›¾ç‰‡
- è¿›è¡Œå›¾åƒåˆ†ç±»
- è¾“å‡ºTop-5é¢„æµ‹ç»“æœ

**ç¤ºä¾‹è¾“å‡ºï¼š**
```
æ­£åœ¨åŠ è½½ViTæ¨¡å‹...
æ­£åœ¨è¿›è¡Œæ¨ç†...
é¢„æµ‹ç±»åˆ«ID: 281
é¢„æµ‹ç±»åˆ«: tabby cat

Top-5 é¢„æµ‹ç»“æœ:
1. tabby cat: 0.4123
2. Egyptian cat: 0.3456
3. tiger cat: 0.1234
...
```

---

## ğŸ”¨ ä»é›¶å®ç°ViTæ¶æ„

### æ–¹æ¡ˆB: ç†è§£å¹¶æµ‹è¯•ViTæ¶æ„

```bash
python vit_from_scratch.py
```

**è¿™ä¸ªè„šæœ¬åŒ…å«ï¼š**
- âœ… Patch Embeddingå±‚
- âœ… Multi-Head Self-Attention
- âœ… Transformer Encoder Block
- âœ… å®Œæ•´çš„ViTæ¨¡å‹
- âœ… ä¸‰ç§æ¨¡å‹å¤§å°ï¼šTiny/Small/Base

**ä»£ç ç»“æ„ï¼š**
```
VisionTransformer
â”œâ”€â”€ PatchEmbedding      # å›¾åƒåˆ‡åˆ†ä¸ºpatches
â”œâ”€â”€ TransformerBlock    # Transformerç¼–ç å™¨
â”‚   â”œâ”€â”€ MultiHeadAttention
â”‚   â””â”€â”€ MLP
â””â”€â”€ Classification Head # åˆ†ç±»å±‚
```

**æ¨¡å‹å‚æ•°å¯¹æ¯”ï¼š**
| æ¨¡å‹ | å‚æ•°é‡ | é€‚ç”¨åœºæ™¯ |
|------|--------|----------|
| ViT-Tiny | ~5M | å¿«é€Ÿå®éªŒ |
| ViT-Small | ~22M | ä¸­ç­‰æ•°æ®é›† |
| ViT-Base | ~86M | å¤§è§„æ¨¡è®­ç»ƒ |

---

## ğŸ‹ï¸ è®­ç»ƒè‡ªå·±çš„æ¨¡å‹

### æ–¹æ¡ˆC: åœ¨CIFAR-10ä¸Šä»å¤´è®­ç»ƒ

```bash
# è®­ç»ƒViT-Tinyæ¨¡å‹
python train_vit_cifar10.py
```

**è®­ç»ƒé…ç½®ï¼š**
- æ•°æ®é›†: CIFAR-10 (60,000å¼ å›¾ç‰‡, 10ç±»)
- æ¨¡å‹: ViT-Tiny
- è®­ç»ƒè½®æ•°: 50 epochs
- æ‰¹æ¬¡å¤§å°: 64
- å­¦ä¹ ç‡: 0.001 (cosine decay)

**é¢„æœŸç»“æœï¼š**
- è®­ç»ƒæ—¶é—´: ~2å°æ—¶ (GPU) / ~10å°æ—¶ (CPU)
- éªŒè¯å‡†ç¡®ç‡: 70-75% (ViT-Tiny)

**æç¤ºï¼š** å¦‚æœGPUå†…å­˜ä¸è¶³ï¼Œå¯ä»¥ï¼š
1. å‡å°batch_size
2. ä½¿ç”¨æ›´å°çš„å›¾åƒå°ºå¯¸
3. å‡å°‘æ¨¡å‹æ·±åº¦

### æ–¹æ¡ˆD: ä½¿ç”¨é¢„è®­ç»ƒæ¨¡å‹å¾®è°ƒ

```bash
# å¾®è°ƒé¢„è®­ç»ƒViTæ¨¡å‹
python finetune_vit.py (+ å‚æ•°è®¾ç½®)
```

**å‚æ•°è¯´æ˜**

1.  `--num_classes`ï¼šæŒ‡å®šåˆ†ç±»çš„ç±»åˆ«æ•°ï¼Œé»˜è®¤å€¼ä¸º 10
2.  `--num_epochs`ï¼šæŒ‡å®šè®­ç»ƒè½®æ•°ï¼Œé»˜è®¤ä¸º 50
3.  `--batch_size`ï¼šæŒ‡å®šè®­ç»ƒæ‰¹æ¬¡å¤§å°ï¼Œé»˜è®¤ä¸º 32
4.  `--model_dir`ï¼šæŒ‡å®šæ¨¡å‹è·¯å¾„ï¼Œå¯ä»¥æ˜¯ huggingface ä¸Šæ¨¡å‹åç§°æˆ–æœ¬åœ°ç›®å½•
5.  `--gpu_ids`ï¼šæŒ‡å®šä½¿ç”¨çš„ GPU ID åˆ—è¡¨ï¼Œæ”¯æŒ DDP å¤šçº¿ç¨‹ï¼Œä¸åŒ ID é—´ç”¨é€—å·éš”å¼€
6.  `--patience`ï¼šæŒ‡å®š Early-Stop çš„è€å¿ƒåº¦ï¼Œé»˜è®¤ä¸º 5
7.  `--full_finetune`ï¼šæŒ‡å®šè¿ç§»å­¦ä¹ ç­–ç•¥ï¼Œyï¼šå…¨å‚å¾®è°ƒï¼›nï¼šåªè®­ç»ƒåˆ†ç±»å¤´

---


## ğŸ¯ ä½¿ç”¨è‡ªå·±çš„æ•°æ®é›†

### å‡†å¤‡æ•°æ®é›†
```
your_dataset/
â”œâ”€â”€ train/
â”‚   â”œâ”€â”€ class1/
â”‚   â”‚   â”œâ”€â”€ img1.jpg
â”‚   â”‚   â””â”€â”€ img2.jpg
â”‚   â””â”€â”€ class2/
â”‚       â”œâ”€â”€ img1.jpg
â”‚       â””â”€â”€ img2.jpg
â””â”€â”€ test/
    â”œâ”€â”€ class1/
    â””â”€â”€ class2/
```

### ä¿®æ”¹ä»£ç 
```python
# å°† finetune_vit.py çš„ get_dataloaders() ä¸­æ•°æ®é›†åŠ è½½é€»è¾‘ä¿®æ”¹ä¸ºï¼š
train_val_dataset = datasets.ImageFolder(
    root='your_dataset/train',
    transform=None
)

test_dataset = datasets.ImageFolder(
    root='your_dataset/test'
    transform=None
)

# transform=None æ˜¯å› ä¸ºåç»­ä¼šå°† train_val_dataset 
# åˆ’åˆ†ä¸ºè®­ç»ƒé›†å’ŒéªŒè¯é›†ï¼Œå¹¶åœ¨åˆ’åˆ†ä¹‹åå†ç»Ÿä¸€è¿›è¡Œå›¾åƒçš„å˜æ¢
```

---

## â“ å¸¸è§é—®é¢˜

### Q1: GPUå†…å­˜ä¸è¶³æ€ä¹ˆåŠï¼Ÿ
**A:** å°è¯•ä»¥ä¸‹æ–¹æ³•ï¼š
```python
# 1. å‡å°æ‰¹æ¬¡å¤§å°
BATCH_SIZE = 32  # æ”¹ä¸º 16 æˆ– 8

# 2. ä½¿ç”¨æ¢¯åº¦ç´¯ç§¯
accumulation_steps = 4
loss = loss / accumulation_steps
loss.backward()
if (step + 1) % accumulation_steps == 0:
    optimizer.step()
    optimizer.zero_grad()

# 3. ä½¿ç”¨æ··åˆç²¾åº¦è®­ç»ƒ
from torch.cuda.amp import autocast, GradScaler
scaler = GradScaler()
```

### Q2: è®­ç»ƒé€Ÿåº¦æ…¢æ€ä¹ˆåŠï¼Ÿ
**A:** ä¼˜åŒ–å»ºè®®ï¼š
- ä½¿ç”¨å¤šGPUè®­ç»ƒï¼š`torch.nn.DataParallel`
- å¢åŠ num_workersï¼š`DataLoader(..., num_workers=4)`
- ä½¿ç”¨æ›´å°çš„æ¨¡å‹ï¼šViT-Tinyä»£æ›¿ViT-Base
- ä½¿ç”¨é¢„è®­ç»ƒæ¨¡å‹å¾®è°ƒ

### Q3: å‡†ç¡®ç‡ä¸é«˜æ€ä¹ˆåŠï¼Ÿ
**A:** æ”¹è¿›æ–¹æ³•ï¼š
1. ä½¿ç”¨é¢„è®­ç»ƒæ¨¡å‹
2. å¢åŠ æ•°æ®å¢å¼º
3. è°ƒæ•´å­¦ä¹ ç‡
4. è®­ç»ƒæ›´å¤šepochs
5. ä½¿ç”¨æ›´å¤§çš„æ¨¡å‹

### Q4: å¦‚ä½•å¯è§†åŒ–æ³¨æ„åŠ›å›¾ï¼Ÿ
**A:** æ·»åŠ ä»¥ä¸‹ä»£ç ï¼š
```python
# è·å–æ³¨æ„åŠ›æƒé‡
attention_weights = model.vit.encoder.layer[-1].attention.self.attention_probs
# å¯è§†åŒ–ï¼ˆéœ€è¦å®‰è£…matplotlibï¼‰
import matplotlib.pyplot as plt
plt.imshow(attention_weights[0, 0].detach().cpu())
plt.show()
```

---

## ğŸ“š è¿›é˜¶é˜…è¯»

### åŸå§‹è®ºæ–‡
- [An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale](https://arxiv.org/abs/2010.11929)

### ç›¸å…³èµ„æº
- HuggingFace ViTæ–‡æ¡£: https://huggingface.co/docs/transformers/model_doc/vit
- PyTorch Image Models: https://github.com/rwightman/pytorch-image-models
- è®ºæ–‡è§£è¯»è§†é¢‘: [æ¨èæœç´¢ç›¸å…³è§†é¢‘]

---

ç¥ä½ å¤ç°é¡ºåˆ©ï¼ğŸŠ
